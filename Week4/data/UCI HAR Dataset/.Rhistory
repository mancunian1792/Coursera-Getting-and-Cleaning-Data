dat1 <- read.table("../data/parsed-brown-first-words-of-sentence.txt",col.names=c("Count","Word"),quote="")
tot1 <- sum(dat1$Count)
idx1 <- order(dat1$Count,decreasing=TRUE)
ylim <- c(0,max(dat1$Count/tot1))
plot.word.freqs(dat1,ylim=ylim)
word1.probs <- dat1$Count/tot1
word1.surprisals <- -log2(word1.probs)
###################################################
### code chunk number 22: brownSecondWords
###################################################
## data obtained from command: tregex '__ !< __ , (__ !< __ !, __)' brown-trees-all.noempties | egrep '^[A-Za-z]*$' | sort  | uniq -c | tail +2  > ~/ling/prob_lx_book/data/parsed-brown-second-words-of-sentence.txt
dat2 <- read.table("../data/parsed-brown-second-words-of-sentence.txt",col.names=c("Count","Word"),quote="")
tot2 <- sum(dat2$Count)
idx2 <- order(dat2$Count,decreasing=TRUE)
plot.word.freqs(dat2,ylim=ylim)
word2.probs <- dat2$Count/tot2
word2.surprisals <- -log2(word2.probs)
###################################################
### code chunk number 23: brownFirstPOS
###################################################
## ddata obtained from command: tregex -u '__ <( __ ! < __)!, __' brown-trees-all.noempties | egrep -o '^[A-Za-z$]*' | sort  | uniq -c | tail +2  > ~/ling/prob_lx_book/data/parsed-brown-first-pos-of-sentence.txt
posDat1 <- read.table("../data/parsed-brown-first-pos-of-sentence.txt",col.names=c("Count","Word"),quote="")
posTot1 <- sum(posDat1$Count)
ylim <- c(0,max(posDat1$Count/posTot1))
plot.word.freqs(posDat1,offset=0.25,ylim=ylim)
pos1.probs <- with(posDat1,tapply(Count,Word,sum))/posTot1
pos1.surprisals <- -log2(pos1.probs)
###################################################
### code chunk number 24: brownSecondPOS
###################################################
## ddata obtained from command: tregex -u '__ < (__ !< __) , (__ !< __ !, __)' brown-trees-all.noempties | egrep -o '^[A-Za-z$]*' | sort  | uniq -c | tail +2  > ~/ling/prob_lx_book/data/parsed-brown-second-pos-of-sentence.txt
posDat2 <- read.table("../data/parsed-brown-second-pos-of-sentence.txt",col.names=c("Count","Word"),quote="")
posTot2 <- sum(posDat2$Count)
plot.word.freqs(posDat2,offset=0.25,ylim=ylim)
pos2.probs <- with(posDat2,tapply(Count,Word,sum))/posTot2
pos2.surprisals <- -log2(pos2.probs)
### R code from vignette source '../univariate_probability_chapter/univariate_probability.Rnw'
###################################################
### code chunk number 1: helperFunction
###################################################
roundN <- function(x,decimals=2,fore=5) sprintf(paste("%",fore,".",decimals,"f",sep=""),x)
###################################################
### code chunk number 2: bernoulliCDF
###################################################
x <- seq(-1,2,by=0.01)
y <- ifelse(x < 0, 0, ifelse(x<1, 0.6, 1))
plot(y ~ x, type ="l", yaxt="n",ylab=expression(F(x)), xlab=expression(x))
axis(2, at=c(0, 0.6, 1), labels=c(0, expression(1-pi), 1),las=1)
###################################################
### code chunk number 3: uniformPDF
###################################################
x <- seq(0, 5, by=0.01)
y <- sapply(x, function(x) ifelse(x > 3 & x < 4, 1, 0))
plot(y ~ x, xaxt="n", yaxt="n",ylim=c(0,1.2),type="l", ylab=expression(p(x)))
axis(1, c(0, 3, 4), c(0, expression(a), expression(b)))
axis(2, c(0, 1), c(0, expression(frac(1,b-a))), las=1)
###################################################
### code chunk number 4: uniformCDF
###################################################
x <- seq(0,5, by=0.01)
y <- sapply(x, function(x) if(x < 3) 0 else if(x < 4) x - 3 else 1)
plot(y ~ x, xaxt="n",ylim=c(0,1.2),type="l",ylab=expression(P(x)))
axis(1, c(0, 3, 4), c(0, expression(a), expression(b)))
###################################################
### code chunk number 5: hertzFrequencyExample
###################################################
x <- seq(0,2000,by=1)
y <- log(x,base=10)
p <- function(x) {ifelse(x >= 100 & x <= 1000,1/900,0)}
plot(x,p(x),type="l",ylim=c(0,0.005),xlab="Frequency (Hertz)",ylab="p(X=x)")
###################################################
### code chunk number 6: hertzFrequencyExampleLog
###################################################
p1 <- function(y) {ifelse(10^y >= 100 & 10^y <= 1000, 1/900 * 10^y * log(10),0)}
plot(y,p1(y),type="l",xlab="log-Hertz",ylab="p(Y=y)")
###################################################
### code chunk number 7: constituentOrderProbsFn
###################################################
probs <- function(g1,g2,g3) {
f <- c(g1*g2*g3,g1*g2*(1-g3),g1*(1-g2)*(1-g3),(1-g1)*g2*g3,(1-g1)*(1-g2)*g3,(1-g1)*(1-g2)*(1-g3))
return(f/sum(f))
}
g1 <- 0.9
g2 <- 0.8
g3 <- 0.5
p <- probs(g1,g2,g3)
freqs <- c(566,488,95,25,11,4)
###################################################
### code chunk number 8: normalPDF
###################################################
musigsq <- function(mu1, s1) substitute(paste(mu,"=",mu1,", ",sigma^2,"=",s1), list(mu1=mu1, s1=s1))
x <- seq(-5,5,by=0.01)
plot(dnorm(x) ~ x, type="l",ylim=c(0,0.85),lwd=3,cex.lab=1.5, ylab=expression(p(x)))
lines(dnorm(x,0,2) ~ x, lty=2, col=3,lwd=3)
lines(dnorm(x,0,1/2) ~ x, lty=3, col=2,lwd=3)
lines(dnorm(x,2) ~ x, lty=4, col="magenta",lwd=3)
legend(-5,0.8,
as.expression(c(musigsq(0,1),musigsq(0,2),musigsq(0,0.5),musigsq(2,1))), lty=c(1,2,3,4), col=c(1,3,2,"magenta"),lwd=3,cex=1.2)
###################################################
### code chunk number 9: normalCDF
###################################################
plot(pnorm(x) ~ x, type="l",ylim=c(0,1),lwd=3,cex.lab=1.5,ylab=expression(P(X<x)))
lines(pnorm(x,0,2) ~ x, lty=2, col=3,lwd=3)
lines(pnorm(x,0,1/2) ~ x, lty=3, col=2,lwd=3)
lines(pnorm(x,2) ~ x, lty=4, col="magenta",lwd=3)
legend(-5,0.8,
as.expression(c(musigsq(0,1),musigsq(0,2),musigsq(0,0.5),musigsq(2,1))), lty=c(1,2,3,4), col=c(1,3,2,"magenta"),lwd=3,cex=1.2)
###################################################
### code chunk number 10: f0rawHistogram
###################################################
xlim.kde <- c(80,210)
set.seed(100)
pb.dat <- read.table("../data/peterson_barney_data/pb.txt",header=T,quote="")
aa <- subset(pb.dat, Vowel=="aa" & Type=="m")
hist(jitter(aa$F0, amount=2.5), breaks=10000, xlab="F0 frequency (Hz)", main="", ylab="Count",cex.lab=1.5,cex.axis=1.5,xlim=xlim.kde)
###################################################
### code chunk number 11: f0histogramStartAt95
###################################################
hist(aa$F0,breaks=seq(95,215, by=5), freq=F, xlab="F0 frequency (Hz)", ylab=expression(p(x)), main="",cex.lab=1.5,cex.axis=1.5,xlim=xlim.kde)
###################################################
### code chunk number 12: f0histogramStartAt94
###################################################
hist(aa$F0,breaks=seq(94,284, by=5), freq=F, xlab="F0 frequency (Hz)", ylab=expression(p(x)), main="",cex.lab=1.5,cex.axis=1.5,xlim=xlim.kde)
###################################################
### code chunk number 13: f0kdeNormal
###################################################
plot(density(aa$F0, bw=5),lty=1,xlab="F0 frequency (Hz)", ylab=expression(p(x)), main="",cex.lab=1.5,cex.axis=1.5,xlim=xlim.kde)
rug(aa$F0)
###################################################
### code chunk number 14: f0kdeRectangular
###################################################
plot(density(aa$F0, kernel="rectangular",bw=20/sqrt(12)),lty=1,xlab="F0 frequency (Hz)", ylab=expression(p(x)), main="",cex.lab=1.5,cex.axis=1.5,xlim=xlim.kde) ## note that in R the "bw" parameter is the standard deviation of the kernel density function, so that for a desired rectangle of width W we use a bandwidth of W/sqrt(12)
rug(aa$F0)
###################################################
### code chunk number 15: crossvalidationClassificationAccuracyExample
###################################################
set.seed(23)
N <- 300
M <- 160
x <- c(rep(1,M),rep(0,N-M))
j <- sample(1:N,N)
total.correct <- 0
yHeldOut <- numeric(4)
acc <- numeric(4)
for(i in 0:3) {
idx <- j[(1:(N/4)) + i*(N/4)]
testing.data <- x[idx]
training.data <- x[-idx]
yHeldOut[i+1] <- sum(testing.data)
##print(c(sum(training.data),sum(training.data==0)))
if(mean(training.data) > 0.5)
acc[i+1] <- sum(testing.data == 1) / N * 4
else
acc[i+1] <- sum(testing.data == 0) / N * 4
}
###################################################
### code chunk number 16: f0kdeMultipleBandwidths
###################################################
plot(density(aa$F0, bw=2),lwd=3,lty=3,col="green",xlab="F0 frequency (Hz)", ylab=expression(p(x)), main="",cex.lab=1.5,cex.axis=1.5)
lines(density(aa$F0, bw=10),lwd=3,lty=2,col="magenta")
lines(density(aa$F0, bw=30),lwd=3,lty=1,col="black")
legend(145,0.03,c("bw=30","bw=10","bw=2"),lwd=3,lty=1:3,col=c("black","magenta","green"),cex=1.5)
rug(aa$F0)
###################################################
### code chunk number 17: likelihoodInKernelEstimation
###################################################
f <- function(x,y,bw) {
d <- density(x,bw=bw,n=300,from=51,to=350) ## set up the grid of positions at which the kernel density is estimated
idx <- sapply(y,function(z) which.max(z==d$x)) ## figure out which position on the grid on which the density is estimated each observation corresponds to
sum(log(d$y[idx]))
}
b <- seq(0.1,30,by=0.1) ## set up the range of bandwidths to be examined
result <- sapply(b, function(bw) f(aa$F0,aa$F0,bw))
###################################################
### code chunk number 18: likelihoodInKernelEstimationPlot
###################################################
plot(b,result,type="l",xlab="Bandwidth",ylab="Log-likelihood")#,ylim=c(-550,-250))
###################################################
### code chunk number 19: likelihoodInKernelEstimationCrossval
###################################################
set.seed(10)
idx <- sample(1:length(aa$F0)) ## creates a random ordering of the integers from 1 to length(aa$F0); each of the K folds is just a contiguous 1/K chunk of this ordering
g <- function(x,bw,idx,folds=6) {
ll <- numeric(folds)
for(i in 1:folds)  {
j <- 1:(length(x)/folds) + (length(x)/folds) * (i-1) ## pull out the i-th contiguous 1/K chunk
training <- x[idx[-j]]
testing <- x[idx[j]]
ll[i] <- f(training,testing,bw=bw)
}
sum(ll)
}
ll.cval <- sapply(b, function(bw) g(aa$F0,bw,idx))
## For too narrow a bandwidth, observations on the right tail will have probability indistinguishable from zero,
## so we exclude these from plotting.
###################################################
### code chunk number 20: likelihoodInKernelEstimationCrossvalPlot
###################################################
plot(b[ll.cval > -Inf],ll.cval[ll.cval > -Inf],type="l",xlab="Bandwidth",ylab="Cross-validated log-likelihood")
###################################################
### code chunk number 21: brownFirstWords
###################################################
plot.word.freqs <- function(dat,offset=10,ylim=NULL) {
tot <- sum(dat$Count)
idx <- order(dat$Count,decreasing=TRUE)
x <- 1:length(idx)
if(is.null(ylim))
ylim <- c(0,1.1*max(dat$Count/tot))
plot(x,(dat$Count/tot)[idx[x]],type='h',lwd=1,ylim=1.1*ylim)
text.x.pos <- 0.9*max(x)*1:10/10
text.y.pos <- dat$Count[idx[1:10]]/tot
arrows(1:10+offset,text.y.pos,text.x.pos,text.y.pos,code=0,col="gray")
text(text.x.pos,text.y.pos,paste(round(-log2(text.y.pos),2),dat$Word[idx[1:10]],sep="\n"),pos=3,offset=0.25)
}
##  Data obtained from command: tregex '__ !< __ !, __' brown-trees-all.noempties | egrep '^[A-Za-z]*$' | sort  | uniq -c | tail +2  > ~/ling/prob_lx_book/data/parsed-brown-first-words-of-sentence.txt
dat1 <- read.table("../data/parsed-brown-first-words-of-sentence.txt",col.names=c("Count","Word"),quote="")
tot1 <- sum(dat1$Count)
idx1 <- order(dat1$Count,decreasing=TRUE)
ylim <- c(0,max(dat1$Count/tot1))
plot.word.freqs(dat1,ylim=ylim)
word1.probs <- dat1$Count/tot1
word1.surprisals <- -log2(word1.probs)
###################################################
### code chunk number 22: brownSecondWords
###################################################
## data obtained from command: tregex '__ !< __ , (__ !< __ !, __)' brown-trees-all.noempties | egrep '^[A-Za-z]*$' | sort  | uniq -c | tail +2  > ~/ling/prob_lx_book/data/parsed-brown-second-words-of-sentence.txt
dat2 <- read.table("../data/parsed-brown-second-words-of-sentence.txt",col.names=c("Count","Word"),quote="")
tot2 <- sum(dat2$Count)
idx2 <- order(dat2$Count,decreasing=TRUE)
plot.word.freqs(dat2,ylim=ylim)
word2.probs <- dat2$Count/tot2
word2.surprisals <- -log2(word2.probs)
###################################################
### code chunk number 23: brownFirstPOS
###################################################
## ddata obtained from command: tregex -u '__ <( __ ! < __)!, __' brown-trees-all.noempties | egrep -o '^[A-Za-z$]*' | sort  | uniq -c | tail +2  > ~/ling/prob_lx_book/data/parsed-brown-first-pos-of-sentence.txt
posDat1 <- read.table("../data/parsed-brown-first-pos-of-sentence.txt",col.names=c("Count","Word"),quote="")
posTot1 <- sum(posDat1$Count)
ylim <- c(0,max(posDat1$Count/posTot1))
plot.word.freqs(posDat1,offset=0.25,ylim=ylim)
pos1.probs <- with(posDat1,tapply(Count,Word,sum))/posTot1
pos1.surprisals <- -log2(pos1.probs)
###################################################
### code chunk number 24: brownSecondPOS
###################################################
## ddata obtained from command: tregex -u '__ < (__ !< __) , (__ !< __ !, __)' brown-trees-all.noempties | egrep -o '^[A-Za-z$]*' | sort  | uniq -c | tail +2  > ~/ling/prob_lx_book/data/parsed-brown-second-pos-of-sentence.txt
posDat2 <- read.table("../data/parsed-brown-second-pos-of-sentence.txt",col.names=c("Count","Word"),quote="")
posTot2 <- sum(posDat2$Count)
plot.word.freqs(posDat2,offset=0.25,ylim=ylim)
pos2.probs <- with(posDat2,tapply(Count,Word,sum))/posTot2
pos2.surprisals <- -log2(pos2.probs)
# Goals: ARMA modeling - estimation, diagnostics, forecasting.
# 0. SETUP DATA
rawdata <- c(-0.21,-2.28,-2.71,2.26,-1.11,1.71,2.63,-0.45,-0.11,4.79,5.07,-2.24,6.46,3.82,4.29,-1.47,2.69,7.95,4.46,7.28,3.43,-3.19,-3.14,-1.25,-0.50,2.25,2.77,6.72,9.17,3.73,6.72,6.04,10.62,9.89,8.23,5.37,-0.10,1.40,1.60,3.40,3.80,3.60,4.90,9.60,18.20,20.60,15.20,27.00,15.42,13.31,11.22,12.77,12.43,15.83,11.44,12.32,12.10,12.02,14.41,13.54,11.36,12.97,10.00,7.20,8.74,3.92,8.73,2.19,3.85,1.48,2.28,2.98,4.21,3.85,6.52,8.16,5.36,8.58,7.00,10.57,7.12,7.95,7.05,3.84,4.93,4.30,5.44,3.77,4.71,3.18,0.00,5.25,4.27,5.14,3.53,4.54,4.70,7.40,4.80,6.20,7.29,7.30,8.38,3.83,8.07,4.88,8.17,8.25,6.46,5.96,5.88,5.03,4.99,5.87,6.78,7.43,3.61,4.29,2.97,2.35,2.49,1.56,2.65,2.49,2.85,1.89,3.05,2.27,2.91,3.94,2.34,3.14,4.11,4.12,4.53,7.11,6.17,6.25,7.03,4.13,6.15,6.73,6.99,5.86,4.19,6.38,6.68,6.58,5.75,7.51,6.22,8.22,7.45,8.00,8.29,8.05,8.91,6.83,7.33,8.52,8.62,9.80,10.63,7.70,8.91,7.50,5.88,9.82,8.44,10.92,11.67)
# Make a R timeseries out of the rawdata: specify frequency & startdate
gIIP <- ts(rawdata, frequency=12, start=c(1991,4))
print(gIIP)
plot.ts(gIIP, type="l", col="blue", ylab="IIP Growth (%)", lwd=2,
main="Full data")
grid()
# Based on this, I decide that 4/1995 is the start of the sensible period.
gIIP <- window(gIIP, start=c(1995,4))
print(gIIP)
plot.ts(gIIP, type="l", col="blue", ylab="IIP Growth (%)", lwd=2,
main="Estimation subset")
grid()
# Descriptive statistics about gIIP
mean(gIIP); sd(gIIP); summary(gIIP);
plot(density(gIIP), col="blue", main="(Unconditional) Density of IIP growth")
acf(gIIP)
# 1. ARMA ESTIMATION
m.ar2 <- arima(gIIP, order = c(2,0,0))
print(m.ar2)                       # Print it out
# 2. ARMA DIAGNOSTICS
tsdiag(m.ar2)                      # His pretty picture of diagnostics
## Time series structure in errors
print(Box.test(m.ar2$residuals, lag=12, type="Ljung-Box"));
## Sniff for ARCH
print(Box.test(m.ar2$residuals^2, lag=12, type="Ljung-Box"));
## Eyeball distribution of residuals
plot(density(m.ar2$residuals), col="blue", xlim=c(-8,8),
main=paste("Residuals of AR(2)"))
# 3. FORECASTING
## Make a picture of the residuals
plot.ts(m.ar2$residual, ylab="Innovations", col="blue", lwd=2)
s <- sqrt(m.ar2$sigma2)
abline(h=c(-s,s), lwd=2, col="lightGray")
p <- predict(m.ar2, n.ahead = 12)         # Make 12 predictions.
print(p)
## Watch the forecastability decay away from fat values to 0.
## sd(x) is the naive sigma. p$se is the prediction se.
gain <- 100*(1-p$se/sd(gIIP))
plot.ts(gain, main="Gain in forecast s.d.", ylab="Per cent",
col="blue", lwd=2)
## Make a pretty picture that puts it all together
ts.plot(gIIP, p$pred, p$pred-1.96*p$se, p$pred+1.96*p$se,
gpars=list(lty=c(1,1,2,2), lwd=c(2,2,1,1),
ylab="IIP growth (%)", col=c("blue","red", "red", "red")))
grid()
abline(h=mean(gIIP), lty=2, lwd=2, col="lightGray")
legend(x="bottomleft", cex=0.8, bty="n",
lty=c(1,1,2,2), lwd=c(2,1,1,2),
col=c("blue", "red", "red", "lightGray"),
legend=c("IIP", "AR(2) forecasts", "95% C.I.", "Mean IIP growth"))
rawdata <- c(-0.21,-2.28,-2.71,2.26,-1.11,1.71,2.63,-0.45,-0.11,4.79,5.07,-2.24,6.46,3.82,4.29,-1.47,2.69,7.95,4.46,7.28,3.43,-3.19,-3.14,-1.25,-0.50,2.25,2.77,6.72,9.17,3.73,6.72,6.04,10.62,9.89,8.23,5.37,-0.10,1.40,1.60,3.40,3.80,3.60,4.90,9.60,18.20,20.60,15.20,27.00,15.42,13.31,11.22,12.77,12.43,15.83,11.44,12.32,12.10,12.02,14.41,13.54,11.36,12.97,10.00,7.20,8.74,3.92,8.73,2.19,3.85,1.48,2.28,2.98,4.21,3.85,6.52,8.16,5.36,8.58,7.00,10.57,7.12,7.95,7.05,3.84,4.93,4.30,5.44,3.77,4.71,3.18,0.00,5.25,4.27,5.14,3.53,4.54,4.70,7.40,4.80,6.20,7.29,7.30,8.38,3.83,8.07,4.88,8.17,8.25,6.46,5.96,5.88,5.03,4.99,5.87,6.78,7.43,3.61,4.29,2.97,2.35,2.49,1.56,2.65,2.49,2.85,1.89,3.05,2.27,2.91,3.94,2.34,3.14,4.11,4.12,4.53,7.11,6.17,6.25,7.03,4.13,6.15,6.73,6.99,5.86,4.19,6.38,6.68,6.58,5.75,7.51,6.22,8.22,7.45,8.00,8.29,8.05,8.91,6.83,7.33,8.52,8.62,9.80,10.63,7.70,8.91,7.50,5.88,9.82,8.44,10.92,11.67)
print rawdata
print-> rawdata
rawdata
help print
help(print)
print(rawdata)
rawdata <- c(-0.21,-2.28,-2.71,2.26,-1.11,1.71,2.63,-0.45,-0.11,4.79,5.07,-2.24,6.46,3.82,4.29,-1.47,2.69,7.95,4.46,7.28,3.43,-3.19,-3.14,-1.25,-0.50,2.25,2.77,6.72,9.17,3.73,6.72,6.04,10.62,9.89,8.23,5.37,-0.10,1.40,1.60,3.40,3.80,3.60,4.90,9.60,18.20,20.60,15.20,27.00,15.42,13.31,11.22,12.77,12.43,15.83,11.44,12.32,12.10,12.02,14.41,13.54,11.36,12.97,10.00,7.20,8.74,3.92,8.73,2.19,3.85,1.48,2.28,2.98,4.21,3.85,6.52,8.16,5.36,8.58,7.00,10.57,7.12,7.95,7.05,3.84,4.93,4.30,5.44,3.77,4.71,3.18,0.00,5.25,4.27,5.14,3.53,4.54,4.70,7.40,4.80,6.20,7.29,7.30,8.38,3.83,8.07,4.88,8.17,8.25,6.46,5.96,5.88,5.03,4.99,5.87,6.78,7.43,3.61,4.29,2.97,2.35,2.49,1.56,2.65,2.49,2.85,1.89,3.05,2.27,2.91,3.94,2.34,3.14,4.11,4.12,4.53,7.11,6.17,6.25,7.03,4.13,6.15,6.73,6.99,5.86,4.19,6.38,6.68,6.58,5.75,7.51,6.22,8.22,7.45,8.00,8.29,8.05,8.91,6.83,7.33,8.52,8.62,9.80,10.63,7.70,8.91,7.50,5.88,9.82,8.44,10.92,11.67)
source("C:\\Users\\user\\Documents\\2.R")
q()
library("manipulate", lib.loc="~/R/win-library/3.1")
library("rstudio", lib.loc="~/R/win-library/3.1")
detach("package:manipulate", unload=TRUE)
detach("package:rstudio", unload=TRUE)
install.packages("lsmeans")
library("lsmeans", lib.loc="~/R/win-library/3.1")
library("swirl")
swirl()
head(flags)
dim(flags)
class(flags)
cls_list<-lapply(flags,class)
cls_lsit
cls_list
class(cls_list)
as.character(cls_list)
cls_vect<-sapply(flags,class)
class(cls_vect)
sum(flags$orange)
flag_colors<-flags[,11:17]
head(flag_colors)
lapply(flag_colors,sum)
sapply(flag_colors,sum)
sapply(flag_colors,mean)
flag_shapes<-flags[,19:23]
lapply(flag_shapes,range)
shape_mat<-sapply(flag_shapes,range)
shape_mat
class(shape_mat)
unique(c(3,4,5,5,5,6,6))
unique_vals<-lapply(flags,unique)
unique_vals
lapply(unique_vals,length)
sapply(unique_vals,length)
sapply(flags,unique)
lapply(unique_vals,function(elem) elem[2])
library("swirl")
swirl()
library(ggvis)
install.packages(ggvis)
install.packages("ggvis")
library(ggvis)
library("ggvis")
install.packages("dplyr")
library(ggvis)
install.packages("dplyr")
library(ggvis)
install.packages("readxl")
install.packages("dplyr")
version()
version
dplyr-master::
?dplyr
??dplyr
install.packages("readxl")
library("readxl")
destloc_q2<-"C:\\Users\\user\\Documents\\Datascience_specialization\\Getting and cleaning data\\Week 1\\data\\quiz\\q2.xlsx"
q2_dwelve<-read_excel(path = destloc_q2,sheet = 1,col_names = FALSE,skip=17)
excel_url<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
destloc_q2<-"C:\\Users\\user\\Documents\\Datascience_specialization\\Getting and cleaning data\\Week 1\\data\\quiz\\q2.xlsx"
download.file(excel_url,destloc_q2)
q2_dwelve<-read_excel(path = destloc_q2,sheet = 1,col_names = FALSE,skip=17)
## Question 2
excel_url<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
destloc_q2<-"C:\\Users\\user\\Documents\\Datascience_specialization\\Getting and cleaning data\\Week 1\\data\\quiz\\q2.xls"
download.file(excel_url,destloc_q2)
## Question 5
q5_url<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
destloc_q3<-"C:\\Users\\user\\Documents\\Datascience_specialization\\Getting and cleaning data\\Week 1\\data\\quiz\\q5.csv"
download.file(q5_url,destloc_q3)
?fread
install.packages("fread")
install.packages("readr")
library("readr")
?fread
??fread
data_5_frame<-read.csv(destloc_q3)
swirl()
library("swirl")
swirl()
swirl()
install_from_swirl("Getting and Cleaning Data")
swirl()
destloc_q2
url<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
destloc_q1_week4<-"C:\\Users\\user\\Documents\\Datascience_specialization\\Getting and cleaning data\\Week 4\\data\\quiz\\q1.csv"
download.file(url,destloc_q1_week4)
date_downloaded<-date()
url<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
destloc_q1_week4<-"C:\\Users\\user\\Documents\\Datascience_specialization\\Getting and cleaning data\\Week 4\\data\\quiz"
download.file(url,destloc_q1_week4)
date_downloaded<-date()
url<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
destloc_q1_week4<-"C:\\Users\\user\\Documents\\Datascience_specialization\\Getting and cleaning data\\Week 4\\data\\quiz\\q1.csv"
download.file(url,destloc_q1_week4)
date_downloaded<-date()
df_q1_week4<-read.csv("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv")
View(df_q1_week4)
View(df_q1_week4)
?strsplit
strsplit(names(df_q1_week4),"wgtp")
class(strsplit(names(df_q1_week4),"wgtp"))
strsplit(names(df_q1_week4),"wgtp")[123]
strsplit(names(df_q1_week4),"wgtp")[123]
df_q2_week4<-read.csv("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv")
View(df_q2_week4)
View(df_q2_week4)
strsplit(df_q2_week4$X.3,",")
strsplit(df_q2_week4$X.3,"\,")
df[] <- lapply(df_q2_week4, function(x) as.numeric(gsub("\\.", "", as.character(x))))
x_3<-df_q2_week4$X.3
x_3<-df_q2_week4$X.3
class(x_3)
x_3<-data.frame(df_q2_week4$X.3)
View(x_3)
View(x_3)
x_3<-as.numeric(gsub(",","",x_3))
class(x_3)
x_3
x_3<-data.frame(df_q2_week4$X.3)
x_3<-(gsub(",","",x_3))
x_3
x_3<-data.frame(df_q2_week4$X.3)
View(x_3)
View(x_3)
getwd()
setwd("C:\Users\user\Documents\Datascience_specialization\Getting and cleaning data\Week4\data\UCI HAR Dataset")
setwd("C:\\Users\\user\\Documents\\Datascience_specialization\\Getting and cleaning data\\Week4\\data\\UCI HAR Dataset")
# Step1. Merges the training and the test sets to create one data set.
setwd("C:\Users\user\Documents\Datascience_specialization\Getting and cleaning data\Week4\data\UCI HAR Dataset")
trainData <- read.table("./data/train/X_train.txt")
dim(trainData) # 7352*561
head(trainData)
trainLabel <- read.table("./data/train/y_train.txt")
table(trainLabel)
trainSubject <- read.table("./data/train/subject_train.txt")
testData <- read.table("./data/test/X_test.txt")
dim(testData) # 2947*561
testLabel <- read.table("./data/test/y_test.txt")
table(testLabel)
testSubject <- read.table("./data/test/subject_test.txt")
joinData <- rbind(trainData, testData)
dim(joinData) # 10299*561
joinLabel <- rbind(trainLabel, testLabel)
dim(joinLabel) # 10299*1
joinSubject <- rbind(trainSubject, testSubject)
dim(joinSubject) # 10299*1
# Step2. Extracts only the measurements on the mean and standard
# deviation for each measurement.
features <- read.table("./data/features.txt")
dim(features)  # 561*2
meanStdIndices <- grep("mean\\(\\)|std\\(\\)", features[, 2])
length(meanStdIndices) # 66
joinData <- joinData[, meanStdIndices]
dim(joinData) # 10299*66
names(joinData) <- gsub("\\(\\)", "", features[meanStdIndices, 2]) # remove "()"
names(joinData) <- gsub("mean", "Mean", names(joinData)) # capitalize M
names(joinData) <- gsub("std", "Std", names(joinData)) # capitalize S
names(joinData) <- gsub("-", "", names(joinData)) # remove "-" in column names
# Step3. Uses descriptive activity names to name the activities in
# the data set
activity <- read.table("./data/activity_labels.txt")
activity[, 2] <- tolower(gsub("_", "", activity[, 2]))
substr(activity[2, 2], 8, 8) <- toupper(substr(activity[2, 2], 8, 8))
substr(activity[3, 2], 8, 8) <- toupper(substr(activity[3, 2], 8, 8))
activityLabel <- activity[joinLabel[, 1], 2]
joinLabel[, 1] <- activityLabel
names(joinLabel) <- "activity"
# Step4. Appropriately labels the data set with descriptive activity
# names.
names(joinSubject) <- "subject"
cleanedData <- cbind(joinSubject, joinLabel, joinData)
dim(cleanedData) # 10299*68
write.table(cleanedData, "merged_data.txt") # write out the 1st dataset
# Step5. Creates a second, independent tidy data set with the average of
# each variable for each activity and each subject.
subjectLen <- length(table(joinSubject)) # 30
activityLen <- dim(activity)[1] # 6
columnLen <- dim(cleanedData)[2]
result <- matrix(NA, nrow=subjectLen*activityLen, ncol=columnLen)
result <- as.data.frame(result)
colnames(result) <- colnames(cleanedData)
row <- 1
for(i in 1:subjectLen) {
for(j in 1:activityLen) {
result[row, 1] <- sort(unique(joinSubject)[, 1])[i]
result[row, 2] <- activity[j, 2]
bool1 <- i == cleanedData$subject
bool2 <- activity[j, 2] == cleanedData$activity
result[row, 3:columnLen] <- colMeans(cleanedData[bool1&bool2, 3:columnLen])
row <- row + 1
}
}
head(result)
write.table(result, "data_with_means.txt") # write out the 2nd dataset
# data <- read.table("./data_with_means.txt")
# data[1:12, 1:3]
write.table(result, "data_with_means.txt",row.names = FALSE) # write out the 2nd dataset
